# üì∞ An√°lise: Integra√ß√£o de Feeds RSS do Siscomex

**Data:** 17/01/2026  
**Status:** üìã **AN√ÅLISE** - Viabilidade t√©cnica e proposta de implementa√ß√£o

---

## üéØ Objetivo

Integrar feeds RSS do Siscomex para que o Maike receba not√≠cias automaticamente e notifique o usu√°rio sobre atualiza√ß√µes importantes.

**Feeds dispon√≠veis:**
- Not√≠cias Siscomex Importa√ß√£o: https://www.gov.br/siscomex/pt-br/noticias/noticias-siscomex-importacao/noticias-siscomex-importacao/RSS
- Not√≠cias Siscomex Sistemas: https://www.gov.br/siscomex/pt-br/noticias/noticias-siscomex-sistemas/noticias-siscomex-sistemas/RSS

---

## ‚úÖ **VIABILIDADE: POSITIVA**

### **1. Infraestrutura Existente**

‚úÖ **Sistema de Notifica√ß√µes:**
- Tabela `notificacoes_processos` no SQLite
- `NotificacaoService` para criar notifica√ß√µes
- Endpoint `/api/notificacoes` para buscar notifica√ß√µes
- Frontend faz polling a cada 30 segundos
- Sistema de TTS (Text-to-Speech) j√° implementado

‚úÖ **Sistema de Agendamento:**
- `ScheduledNotificationsService` usando `APScheduler`
- J√° tem jobs agendados (resumos di√°rios, lembretes)
- Usa `BackgroundScheduler` com `CronTrigger`

‚úÖ **Sincroniza√ß√£o Autom√°tica:**
- `ProcessoKanbanService` j√° tem sincroniza√ß√£o em background
- Usa threads para rodar periodicamente

### **2. Bibliotecas Necess√°rias**

‚úÖ **feedparser** (n√£o est√° no `requirements.txt`, mas √© leve e confi√°vel):
- Biblioteca Python padr√£o para parsing de RSS/Atom
- Suporta RSS 2.0, Atom, e outros formatos
- Tratamento de encoding autom√°tico
- Extra√ß√£o de t√≠tulo, descri√ß√£o, link, data de publica√ß√£o

‚úÖ **requests** (j√° est√° no `requirements.txt`):
- Para fazer HTTP GET nos feeds RSS

---

## üìä **COMPLEXIDADE: BAIXA-M√âDIA**

### **Complexidade T√©cnica: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2/5)**

**Fatores que facilitam:**
- ‚úÖ Infraestrutura de notifica√ß√µes j√° existe
- ‚úÖ Sistema de agendamento j√° existe
- ‚úÖ Biblioteca `feedparser` √© simples de usar
- ‚úÖ RSS √© um formato padronizado

**Fatores que complicam:**
- ‚ö†Ô∏è Detec√ß√£o de duplicatas (evitar notificar a mesma not√≠cia)
- ‚ö†Ô∏è Filtragem inteligente (quais not√≠cias s√£o relevantes?)
- ‚ö†Ô∏è Tratamento de erros (feed indispon√≠vel, timeout, etc.)
- ‚ö†Ô∏è Armazenamento de hist√≥rico (quais not√≠cias j√° foram processadas?)

### **Complexidade de Neg√≥cio: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2/5)**

**Decis√µes necess√°rias:**
1. **Frequ√™ncia de verifica√ß√£o:** A cada 1h? 2h? 4h?
2. **Filtragem:** Notificar todas as not√≠cias ou apenas as relevantes?
3. **Prioriza√ß√£o:** Algumas not√≠cias s√£o mais importantes que outras?
4. **Hist√≥rico:** Quanto tempo manter hist√≥rico de not√≠cias processadas?

---

## üèóÔ∏è **ARQUITETURA PROPOSTA**

### **Componentes Necess√°rios**

#### **1. `services/rss_siscomex_service.py`** (NOVO)
**Responsabilidade:** Buscar e processar feeds RSS do Siscomex

**Funcionalidades:**
- `buscar_feed_rss(url)`: Faz HTTP GET e parseia RSS
- `extrair_noticias(feed)`: Extrai lista de not√≠cias do feed
- `filtrar_noticias_relevantes(noticias)`: Filtra not√≠cias relevantes (opcional)
- `verificar_duplicata(noticia)`: Verifica se not√≠cia j√° foi processada
- `processar_novas_noticias()`: M√©todo principal que busca, filtra e cria notifica√ß√µes

**Estrutura de dados:**
```python
{
    'titulo': str,
    'descricao': str,
    'link': str,
    'data_publicacao': datetime,
    'fonte': str,  # 'siscomex_importacao' ou 'siscomex_sistemas'
    'guid': str,  # ID √∫nico da not√≠cia (para detec√ß√£o de duplicatas)
}
```

#### **2. Tabela SQLite: `noticias_siscomex`** (NOVO)
**Responsabilidade:** Armazenar hist√≥rico de not√≠cias processadas

**Schema:**
```sql
CREATE TABLE IF NOT EXISTS noticias_siscomex (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    guid TEXT UNIQUE NOT NULL,  -- ID √∫nico da not√≠cia (evita duplicatas)
    titulo TEXT NOT NULL,
    descricao TEXT,
    link TEXT NOT NULL,
    data_publicacao TIMESTAMP,
    fonte TEXT NOT NULL,  -- 'siscomex_importacao' ou 'siscomex_sistemas'
    notificada BOOLEAN DEFAULT 0,  -- Se j√° foi criada notifica√ß√£o
    criado_em TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_guid (guid),
    INDEX idx_fonte_data (fonte, data_publicacao DESC)
);
```

#### **3. Integra√ß√£o com `ScheduledNotificationsService`**
**Responsabilidade:** Agendar verifica√ß√£o peri√≥dica de feeds RSS

**Modifica√ß√µes:**
- Adicionar job no `ScheduledNotificationsService`:
  ```python
  # Verificar feeds RSS a cada 2 horas
  self.scheduler.add_job(
      func=self._verificar_feeds_rss,
      trigger=IntervalTrigger(hours=2),
      id='rss_siscomex_2h',
      name='Verificar Feeds RSS Siscomex',
      replace_existing=True
  )
  ```

#### **4. Integra√ß√£o com `NotificacaoService`**
**Responsabilidade:** Criar notifica√ß√µes para novas not√≠cias

**Tipo de notifica√ß√£o:**
- `tipo_notificacao`: `'noticia_siscomex'`
- `processo_referencia`: `'SISCOMEX'`
- `titulo`: T√≠tulo da not√≠cia
- `mensagem`: Descri√ß√£o da not√≠cia + link
- `dados_extras`: `{'link': url, 'fonte': 'siscomex_importacao' ou 'siscomex_sistemas'}`

---

## üîç **DESAFIOS E SOLU√á√ïES**

### **1. Detec√ß√£o de Duplicatas**

**Problema:** Evitar notificar a mesma not√≠cia m√∫ltiplas vezes.

**Solu√ß√£o:**
- Usar `guid` (ID √∫nico) do RSS como chave √∫nica
- Armazenar `guid` na tabela `noticias_siscomex` com `UNIQUE`
- Antes de criar notifica√ß√£o, verificar se `guid` j√° existe
- Se existir, pular (n√£o criar notifica√ß√£o)

**Implementa√ß√£o:**
```python
def verificar_duplicata(self, guid: str) -> bool:
    """Verifica se not√≠cia j√° foi processada"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT COUNT(*) FROM noticias_siscomex WHERE guid = ?', (guid,))
    count = cursor.fetchone()[0]
    conn.close()
    return count > 0
```

### **2. Filtragem Inteligente (Opcional)**

**Problema:** Nem todas as not√≠cias s√£o relevantes para o usu√°rio.

**Solu√ß√£o (Fase 1 - Simples):**
- Notificar todas as not√≠cias (sem filtragem)
- Usu√°rio pode marcar como "n√£o relevante" no futuro

**Solu√ß√£o (Fase 2 - Avan√ßada):**
- Usar IA para classificar relev√¢ncia
- Palavras-chave: "DUIMP", "DI", "importa√ß√£o", "desembara√ßo", "AFRMM", etc.
- Score de relev√¢ncia (0-1)
- S√≥ notificar se score > 0.7

**Implementa√ß√£o (Fase 2):**
```python
def filtrar_noticias_relevantes(self, noticias: List[Dict]) -> List[Dict]:
    """Filtra not√≠cias relevantes usando palavras-chave"""
    palavras_chave = ['DUIMP', 'DI', 'importa√ß√£o', 'desembara√ßo', 'AFRMM', 
                     'Siscomex', 'Portal √önico', 'Integra Comex']
    
    noticias_relevantes = []
    for noticia in noticias:
        titulo = noticia.get('titulo', '').upper()
        descricao = noticia.get('descricao', '').upper()
        texto_completo = f"{titulo} {descricao}"
        
        # Contar palavras-chave encontradas
        score = sum(1 for palavra in palavras_chave if palavra.upper() in texto_completo)
        
        # Se encontrou pelo menos 1 palavra-chave, √© relevante
        if score > 0:
            noticia['score_relevancia'] = score
            noticias_relevantes.append(noticia)
    
    return noticias_relevantes
```

### **3. Tratamento de Erros**

**Problema:** Feed pode estar indispon√≠vel, timeout, formato inv√°lido, etc.

**Solu√ß√£o:**
- Try/except em todas as opera√ß√µes
- Logging detalhado de erros
- Retry com backoff exponencial (opcional)
- N√£o bloquear outras notifica√ß√µes se RSS falhar

**Implementa√ß√£o:**
```python
def buscar_feed_rss(self, url: str) -> Optional[Dict]:
    """Busca feed RSS com tratamento de erros"""
    try:
        import feedparser
        import requests
        
        # Timeout de 10 segundos
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        feed = feedparser.parse(response.content)
        
        if feed.bozo:  # Erro de parsing
            logger.warning(f"‚ö†Ô∏è Erro ao parsear RSS: {feed.bozo_exception}")
            return None
        
        return feed
    except requests.exceptions.Timeout:
        logger.error(f"‚ùå Timeout ao buscar feed RSS: {url}")
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Erro HTTP ao buscar feed RSS: {url} - {e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado ao buscar feed RSS: {url} - {e}", exc_info=True)
        return None
```

### **4. Armazenamento de Hist√≥rico**

**Problema:** Quanto tempo manter hist√≥rico de not√≠cias processadas?

**Solu√ß√£o:**
- Manter hist√≥rico por 90 dias (configur√°vel)
- Limpeza autom√°tica de not√≠cias antigas
- Job agendado para limpeza semanal

**Implementa√ß√£o:**
```python
def limpar_noticias_antigas(self, dias_retencao: int = 90):
    """Remove not√≠cias mais antigas que X dias"""
    from datetime import datetime, timedelta
    from db_manager import get_db_connection
    
    limite = datetime.now() - timedelta(days=dias_retencao)
    
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        'DELETE FROM noticias_siscomex WHERE data_publicacao < ?',
        (limite.isoformat(),)
    )
    removidas = cursor.rowcount
    conn.commit()
    conn.close()
    
    logger.info(f"üßπ Limpeza de not√≠cias antigas: {removidas} not√≠cias removidas")
```

---

## üìã **PLANO DE IMPLEMENTA√á√ÉO**

### **Fase 1: MVP (M√≠nimo Vi√°vel) - 2-3 horas**

**Objetivo:** Notificar todas as not√≠cias do Siscomex sem filtragem.

**Tarefas:**
1. ‚úÖ Adicionar `feedparser` ao `requirements.txt`
2. ‚úÖ Criar tabela `noticias_siscomex` no `db_manager.py`
3. ‚úÖ Criar `services/rss_siscomex_service.py` com:
   - `buscar_feed_rss(url)`
   - `extrair_noticias(feed)`
   - `verificar_duplicata(guid)`
   - `processar_novas_noticias()`
4. ‚úÖ Integrar com `ScheduledNotificationsService` (verificar a cada 2h)
5. ‚úÖ Integrar com `NotificacaoService` (criar notifica√ß√µes)
6. ‚úÖ Testar com feeds reais

**Crit√©rios de sucesso:**
- ‚úÖ Notifica√ß√µes aparecem no frontend quando h√° novas not√≠cias
- ‚úÖ N√£o cria notifica√ß√µes duplicadas
- ‚úÖ Funciona mesmo se feed estiver temporariamente indispon√≠vel

### **Fase 2: Filtragem Inteligente (Opcional) - 1-2 horas**

**Objetivo:** Filtrar not√≠cias relevantes usando palavras-chave.

**Tarefas:**
1. ‚úÖ Implementar `filtrar_noticias_relevantes()` com palavras-chave
2. ‚úÖ Adicionar score de relev√¢ncia
3. ‚úÖ Configurar threshold (ex: score > 0.7)
4. ‚úÖ Testar com not√≠cias reais

**Crit√©rios de sucesso:**
- ‚úÖ Notifica apenas not√≠cias relevantes
- ‚úÖ Score de relev√¢ncia √© calculado corretamente

### **Fase 3: Limpeza Autom√°tica (Opcional) - 30 minutos**

**Objetivo:** Limpar not√≠cias antigas automaticamente.

**Tarefas:**
1. ‚úÖ Implementar `limpar_noticias_antigas()`
2. ‚úÖ Adicionar job agendado semanal
3. ‚úÖ Testar limpeza

**Crit√©rios de sucesso:**
- ‚úÖ Not√≠cias antigas s√£o removidas automaticamente
- ‚úÖ N√£o remove not√≠cias recentes

---

## üß™ **TESTES**

### **Testes Unit√°rios**

```python
# tests/test_rss_siscomex_service.py

def test_buscar_feed_rss():
    """Testa busca de feed RSS"""
    service = RssSiscomexService()
    feed = service.buscar_feed_rss("https://www.gov.br/siscomex/.../RSS")
    assert feed is not None
    assert 'entries' in feed

def test_extrair_noticias():
    """Testa extra√ß√£o de not√≠cias do feed"""
    service = RssSiscomexService()
    feed = service.buscar_feed_rss("...")
    noticias = service.extrair_noticias(feed)
    assert len(noticias) > 0
    assert 'titulo' in noticias[0]
    assert 'guid' in noticias[0]

def test_verificar_duplicata():
    """Testa detec√ß√£o de duplicatas"""
    service = RssSiscomexService()
    guid = "test-guid-123"
    
    # Primeira vez: n√£o √© duplicata
    assert not service.verificar_duplicata(guid)
    
    # Salvar not√≠cia
    service._salvar_noticia({'guid': guid, ...})
    
    # Segunda vez: √© duplicata
    assert service.verificar_duplicata(guid)
```

### **Testes de Integra√ß√£o**

```python
# tests/test_rss_integracao.py

def test_processar_novas_noticias():
    """Testa processamento completo de novas not√≠cias"""
    service = RssSiscomexService()
    
    # Processar feeds
    novas = service.processar_novas_noticias()
    
    # Verificar se notifica√ß√µes foram criadas
    from services.notificacao_service import NotificacaoService
    notif_service = NotificacaoService()
    notificacoes = notif_service.buscar_notificacoes(
        tipo='noticia_siscomex',
        limite=10
    )
    
    assert len(notificacoes) > 0
```

---

## üìä **ESTIMATIVA DE ESFOR√áO**

| Fase | Tarefas | Tempo Estimado |
|------|---------|----------------|
| **Fase 1: MVP** | Implementa√ß√£o b√°sica | 2-3 horas |
| **Fase 2: Filtragem** | Filtragem inteligente | 1-2 horas |
| **Fase 3: Limpeza** | Limpeza autom√°tica | 30 minutos |
| **Total** | | **3.5-5.5 horas** |

---

## üéØ **RECOMENDA√á√ÉO**

‚úÖ **RECOMENDO IMPLEMENTAR** - Viabilidade alta, complexidade baixa-m√©dia, esfor√ßo moderado.

**Vantagens:**
- ‚úÖ Infraestrutura j√° existe (notifica√ß√µes, agendamento)
- ‚úÖ Biblioteca `feedparser` √© confi√°vel e simples
- ‚úÖ Valor agregado alto (usu√°rio fica informado sobre mudan√ßas no Siscomex)
- ‚úÖ Baixo risco (n√£o afeta funcionalidades existentes)

**Riscos:**
- ‚ö†Ô∏è Feed pode estar indispon√≠vel temporariamente (tratado com try/except)
- ‚ö†Ô∏è Pode gerar muitas notifica√ß√µes (mitigado com filtragem opcional)
- ‚ö†Ô∏è Requer manuten√ß√£o se estrutura do RSS mudar (baixa probabilidade)

**Pr√≥ximos passos:**
1. Confirmar se usu√°rio quer implementar
2. Decidir frequ√™ncia de verifica√ß√£o (recomendo 2h)
3. Decidir se quer filtragem inteligente (Fase 1 ou Fase 2)
4. Implementar Fase 1 (MVP)

---

## üìö **REFER√äNCIAS**

- **feedparser:** https://pythonhosted.org/feedparser/
- **APScheduler:** https://apscheduler.readthedocs.io/
- **RSS 2.0 Spec:** https://www.rssboard.org/rss-specification

---

**√öltima atualiza√ß√£o:** 17/01/2026
